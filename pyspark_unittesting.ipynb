{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import FloatType, IntegerType, BooleanType\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "import unittest\n",
    "import logging\n",
    "import pytest\n",
    "\n",
    "import sys\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'USCND64648K6.CSCMWS.CSCMWS.COM'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '62420'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1539176621006')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://USCND64648K6.CSCMWS.CSCMWS.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark AppName Updated</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b870c28b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = [('spark.app.name', \"Spark AppName Updated\")]\n",
    "conf = spark.sparkContext._conf.setAll(conf)\n",
    "spark.sparkContext.stop()\n",
    "spark = spark.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PySparkTest(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def suppress_py4j_logging(cls):\n",
    "        logger = logging.getLogger('py4j')\n",
    "        logger.setLevel(logging.WARN)\n",
    "\n",
    "    @classmethod\n",
    "    def create_testing_pyspark_session(cls):\n",
    "        return (SparkSession.builder\n",
    "     .master('local[2]')\n",
    "     .appName('my-local-testing-pyspark-context')\n",
    "     .enableHiveSupport()\n",
    "     .getOrCreate())\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.suppress_py4j_logging()\n",
    "        cls.spark = cls.create_testing_pyspark_session()\n",
    "        \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTest(PySparkTest):\n",
    "    \n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "        \n",
    "        \n",
    "    def test_lower(self):\n",
    "        self.assertEqual(\"FOO\".lower(), 'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AdvancedTest(PySparkTest):\n",
    "\n",
    "    def test_rdd(self):\n",
    "        test_rdd = self.spark.sparkContext.parallelize(\n",
    "            ['cat dog mouse', 'cat cat dog'], 2)\n",
    "        results = test_rdd.flatMap(lambda line: line.split()).map(\n",
    "            lambda word: (word, 1)).reduceByKey(add).collect()\n",
    "        expected_results = [('cat', 3), ('dog', 2), ('mouse', 1)]\n",
    "        self.assertEqual(set(results), set(expected_results))\n",
    "        \n",
    "    def assert_frame_equal_with_sort(self, results, expected, keycolumns):\n",
    "        results_sorted = results.sort_values(by=keycolumns).reset_index(drop=True)\n",
    "        expected_sorted = expected.sort_values(by=keycolumns).reset_index(drop=True)\n",
    "        assert_frame_equal(results_sorted, expected_sorted)\n",
    "        \n",
    "    @staticmethod\n",
    "    def my_spark_function(df):\n",
    "        return df[df['make'].isin([\"Rover\", \"Lotus\", \"MINI\"])]\n",
    "        \n",
    "\n",
    "    def test_dataFrame(self):\n",
    "     # Create the test data, with larger examples this can come from a CSV file\n",
    "     # and we can use pd.read_csv(…)\n",
    "        data_pandas = pd.DataFrame({'make': ['Jaguar', 'MG', 'MINI', 'Rover', 'Lotus'],\\\n",
    "                                    'registration': ['AB98ABCD', 'BC99BCDF', 'CD00CDE', 'DE01DEF', 'EF02EFG'],\\\n",
    "                                    'year': [1998, 1999, 2000, 2001, 2002]})\n",
    "    # Turn the data into a Spark DataFrame, self.spark comes from our PySparkTest base class\n",
    "        data_spark = self.spark.createDataFrame(data_pandas)\n",
    "    # Invoke the unit we’d like to test\n",
    "        results_spark = self.my_spark_function(data_spark)\n",
    "     # Turn the results back to Pandas\n",
    "        results_pandas = results_spark.toPandas()\n",
    "    # Our expected results crafted by hand, again, this could come from a CSV\n",
    "     # in case of a bigger example\n",
    "        expected_results = pd.DataFrame({'make':['Rover', 'Lotus', 'MINI'],\n",
    "                                         'registration':['DE01DEF','EF02EFG', 'CD00CDE'],\n",
    "                                         'year':[2001,2002, 2000]})\n",
    "    # Assert that the 2 results are the same. We’ll cover this function in a bit\n",
    "        self.assert_frame_equal_with_sort(results_pandas, expected_results, ['registration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[make: string, registration: string, year: bigint]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asdralias2\\appdata\\local\\programs\\python\\python35\\lib\\socket.py:647: ResourceWarning: unclosed <socket.socket fd=1228, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 62567), raddr=('127.0.0.1', 62565)>\n",
      "  self._sock = None\n",
      ".c:\\users\\asdralias2\\appdata\\local\\programs\\python\\python35\\lib\\socket.py:647: ResourceWarning: unclosed <socket.socket fd=1940, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 62578), raddr=('127.0.0.1', 62576)>\n",
      "  self._sock = None\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 19.216s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
